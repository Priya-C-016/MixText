# -*- coding: utf-8 -*-
"""PyhtonLab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17SgnPIk65dp7Pu922hYaYZwiD0C_SHOb
"""

import warnings
warnings.filterwarnings("ignore")

import pandas as pd
import requests
import json
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

# Load train and test datasets
train_url = "https://raw.githubusercontent.com/Dongping-Chen/MixSet/main/data/MixSet/Mixset_train.json"
test_url = "https://raw.githubusercontent.com/Dongping-Chen/MixSet/main/data/MixSet/Mixset_test.json"
train_data = json.loads(requests.get(train_url).text)
test_data = json.loads(requests.get(test_url).text)

# Convert to DataFrames
train_df = pd.DataFrame(train_data)
test_df = pd.DataFrame(test_data)

# Map the binary column to 0 (Human Written Text) and 1 (Machine Generated Text)
binary_map = {'MGT': 1, 'HWT': 0}
train_df['binary_label'] = train_df['binary'].map(binary_map)
test_df['binary_label'] = test_df['binary'].map(binary_map)

# Prepare features and labels
X_train = train_df[['original', 'revised']]
y_train = train_df['binary_label']
X_test = test_df[['original', 'revised']]
y_test = test_df['binary_label']

# Concatenate original and revised text for training
train_texts = X_train['original'] + " " + X_train['revised']
test_texts = X_test['original'] + " " + X_test['revised']

# Initialize TF-IDF Vectorizer
tfidf = TfidfVectorizer(stop_words='english', max_features=10000)

# Fit and transform the train data, and transform the test data
X_train_tfidf = tfidf.fit_transform(train_texts)
X_test_tfidf = tfidf.transform(test_texts)

# Initialize the Logistic Regression Model
model = LogisticRegression(max_iter=1000)

# Train the model
model.fit(X_train_tfidf, y_train)

# Prediction function
def predict_text_class(original_text, revised_text):
    input_text = original_text + " " + revised_text
    input_tfidf = tfidf.transform([input_text])
    prediction = model.predict(input_tfidf)

    if prediction == 1:
        return "Machine-Generated (MGT)"
    else:
        return "Human-Written (HWT)"

train_df.head()

# Take input from the user
original_text = input("Enter the original text: ")
revised_text = input("Enter the revised text: ")

# Get the prediction
result = predict_text_class(original_text, revised_text)

# Display the result
print(f"Prediction: {result}")

!pip install torch torchvision torchaudio

!pip install transformers datasets

# !pip install transformers datasets

from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset

# Combine original and revised into one field
train_df['text'] = train_df['original'] + " " + train_df['revised']
test_df['text'] = test_df['original'] + " " + test_df['revised']

# Create Hugging Face datasets
train_dataset = Dataset.from_pandas(train_df[['text', 'binary_label']])
test_dataset = Dataset.from_pandas(test_df[['text', 'binary_label']])

# Load tokenizer and model
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

# Tokenization function
def tokenize(example):
    return tokenizer(example['text'], padding="max_length", truncation=True)

train_dataset = train_dataset.map(tokenize, batched=True)
test_dataset = test_dataset.map(tokenize, batched=True)

train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'binary_label'])
test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'binary_label'])

# Training arguments
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    logging_dir="./logs",
    logging_steps=10,
    load_best_model_at_end=True,
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)

# Train
trainer.train()

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import torch.nn.functional as F

# Load pre-trained RoBERTa-based model and tokenizer
model_name = "roberta-base-openai-detector"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# Set the model to evaluation mode (no gradients required)
model.eval()

# Function to detect human-written or machine-generated text
def detect_text(input_text):
    # Tokenize the input text
    inputs = tokenizer(input_text, return_tensors='pt', padding=True, truncation=True, max_length=512)

    # Move tensors to the same device as the model
    inputs = {key: value.to(model.device) for key, value in inputs.items()}

    # Perform the forward pass
    with torch.no_grad():
        logits = model(**inputs).logits

    # Apply softmax to get probabilities
    probabilities = F.softmax(logits, dim=-1)

    # Get the predicted label
    prediction = torch.argmax(probabilities, dim=-1).item()

    # Map the prediction to the corresponding label
    if prediction == 0:
        return "Human-Written (HWT)", probabilities[0][0].item() * 100  # Confidence for HWT
    else:
        return "Machine-Generated (MGT)", probabilities[0][1].item() * 100  # Confidence for MGT

# Example usage
input_text = "The quick brown fox jumps over the lazy dog."
label, confidence = detect_text(input_text)
print(f"Prediction: {label} (Confidence: {confidence:.2f}%)")

# Take input from the user
original_text = input("Enter the original text: ")
revised_text = input("Enter the revised text: ")

# Get the prediction
result = predict_text_class(original_text, revised_text)

# Display the result
print(f"Prediction: {result}")

# model used roberta-base-openai-detector (

import pandas as pd
import requests
import json
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

# Load train and test datasets
train_url = "https://raw.githubusercontent.com/Dongping-Chen/MixSet/main/data/MixSet/Mixset_train.json"
test_url = "https://raw.githubusercontent.com/Dongping-Chen/MixSet/main/data/MixSet/Mixset_test.json"

train_data = json.loads(requests.get(train_url).text)
test_data = json.loads(requests.get(test_url).text)

# Convert to DataFrames
train_df = pd.DataFrame(train_data)
test_df = pd.DataFrame(test_data)

# Map the binary column to 0 (Human Written Text) and 1 (Machine Generated Text)
binary_map = {'MGT': 1, 'HWT': 0}
train_df['binary_label'] = train_df['binary'].map(binary_map)
test_df['binary_label'] = test_df['binary'].map(binary_map)

# Prepare features and labels
X_train = train_df[['original', 'revised']]
y_train = train_df['binary_label']
X_test = test_df[['original', 'revised']]
y_test = test_df['binary_label']

# Concatenate original and revised text for training
train_texts = X_train['original'] + " " + X_train['revised']
test_texts = X_test['original'] + " " + X_test['revised']

# Initialize TF-IDF Vectorizer
tfidf = TfidfVectorizer(stop_words='english', max_features=10000)

# Fit and transform the train data, and transform the test data
X_train_tfidf = tfidf.fit_transform(train_texts)
X_test_tfidf = tfidf.transform(test_texts)

# Initialize the Logistic Regression Model
model = LogisticRegression(max_iter=1000)

# Train the model
model.fit(X_train_tfidf, y_train)

# Prediction function
def predict_text_class(original_text, revised_text):
    input_text = original_text + " " + revised_text
    input_tfidf = tfidf.transform([input_text])
    prediction = model.predict(input_tfidf)

    if prediction == 1:
        return "Machine-Generated (MGT)"
    else:
        return "Human-Written (HWT)"

# Import Hugging Face Transformers
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset

# Combine original and revised into one field
train_df['text'] = train_df['original'] + " " + train_df['revised']
test_df['text'] = test_df['original'] + " " + test_df['revised']

# Create Hugging Face datasets
train_dataset = Dataset.from_pandas(train_df[['text', 'binary_label']])
test_dataset = Dataset.from_pandas(test_df[['text', 'binary_label']])

# Load tokenizer and model
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

# Tokenization function
def tokenize(example):
    return tokenizer(example['text'], padding="max_length", truncation=True)

train_dataset = train_dataset.map(tokenize, batched=True)
test_dataset = test_dataset.map(tokenize, batched=True)

train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'binary_label'])
test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'binary_label'])

# Training arguments
training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    logging_dir="./logs",
    logging_steps=10,
    load_best_model_at_end=True,
    # evaluation_strategy="steps",  # Change to 'steps' or use eval_steps
    # eval_steps=500,  # Adjust this based on your dataset size
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)

# Train the model
trainer.train()


# Load pre-trained RoBERTa-based model and tokenizer
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import torch.nn.functional as F

model_name = "roberta-base-openai-detector"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# Set the model to evaluation mode (no gradients required)
model.eval()

# Function to detect human-written or machine-generated text
def detect_text(input_text):
    # Tokenize the input text
    inputs = tokenizer(input_text, return_tensors='pt', padding=True, truncation=True, max_length=512)

    # Move tensors to the same device as the model
    inputs = {key: value.to(model.device) for key, value in inputs.items()}

    # Perform the forward pass
    with torch.no_grad():
        logits = model(**inputs).logits

    # Apply softmax to get probabilities
    probabilities = F.softmax(logits, dim=-1)

    # Get the predicted label
    prediction = torch.argmax(probabilities, dim=-1).item()

    # Map the prediction to the corresponding label
    if prediction == 0:
        return "Human-Written (HWT)", probabilities[0][0].item() * 100  # Confidence for HWT
    else:
        return "Machine-Generated (MGT)", probabilities[0][1].item() * 100  # Confidence for MGT

# Example usage
input_text = "The quick brown fox jumps over the lazy dog."
label, confidence = detect_text(input_text)
print(f"Prediction: {label} (Confidence: {confidence:.2f}%)")

# Take input from the user
original_text = input("Enter the original text: ")
revised_text = input("Enter the revised text: ")

# Get the prediction
result = predict_text_class(original_text, revised_text)

# Display the result
print(f"Prediction: {result}")

pip install --upgrade transformers

# Take input from the user
original_text = input("Enter the original text: ")
revised_text = input("Enter the revised text: ")

# Get the prediction
result = predict_text_class(original_text, revised_text)

# Display the result
print(f"Prediction: {result}")

